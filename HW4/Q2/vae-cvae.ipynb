{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align: center\">\nDeep Learning HW3 </br>\nVAE and CVAE\n</h1>\n","metadata":{"id":"nTuaIdGJEqRc"}},{"cell_type":"markdown","source":"### Full Name: Borna khodabandeh\n### Student ID: 400109898","metadata":{"id":"C5MiRbbuGbmA"}},{"cell_type":"markdown","source":"# Import necessary libraries","metadata":{"id":"4cWlBsdIJiV8"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torchvision","metadata":{"id":"2Xls_DbtHydf","execution":{"iopub.status.busy":"2024-01-03T15:52:48.826115Z","iopub.execute_input":"2024-01-03T15:52:48.826381Z","iopub.status.idle":"2024-01-03T15:52:52.019952Z","shell.execute_reply.started":"2024-01-03T15:52:48.826355Z","shell.execute_reply":"2024-01-03T15:52:52.019001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the MNIST dataset and data loader","metadata":{"id":"RyoKovGiJo_U"}},{"cell_type":"code","source":"import os\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n# I added the num_workers=os.cpu_count(), pin_memory=False statements\nbatch_size = 100\nx_dim = 28 * 28\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{"id":"yMD2yD-yJvXz"}},{"cell_type":"code","source":"# Function to display an image\ndef show_image(image, figsize=(5, 5)):\n    plt.figure(figsize=figsize)\n    plt.imshow(image, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\n# Function to show random images from each class\ndef show_random_images_from_each_class(dataset, num_images_per_class=4, figsize=(10, 20)):\n    class_labels = list(range(10))  \n    fig, axs = plt.subplots(len(class_labels), num_images_per_class, figsize=figsize) \n\n    for i, label in enumerate(class_labels):\n        class_indices = [idx for idx, target in enumerate(dataset.targets) if target == label]\n        random_indices = random.sample(class_indices, num_images_per_class)\n\n        for j, idx in enumerate(random_indices):\n            image, target = dataset[idx]\n            axs[i, j].imshow(image[0], cmap='gray')\n            axs[i, j].set_title(f\"Class {label}\", fontsize=16)\n            axs[i, j].axis('off')\n            axs[i, j].set_aspect('equal')  \n            axs[i, j].grid(True)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"HxHAgFJfJx28","execution":{"iopub.status.busy":"2024-01-03T15:52:52.963392Z","iopub.execute_input":"2024-01-03T15:52:52.963687Z","iopub.status.idle":"2024-01-03T15:52:52.973125Z","shell.execute_reply.started":"2024-01-03T15:52:52.963659Z","shell.execute_reply":"2024-01-03T15:52:52.972335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the function to show random images from each class\nshow_random_images_from_each_class(train_dataset, num_images_per_class=4)","metadata":{"id":"RRWLPvTFJ2pS","execution":{"iopub.status.busy":"2024-01-03T15:52:52.975215Z","iopub.execute_input":"2024-01-03T15:52:52.975557Z","iopub.status.idle":"2024-01-03T15:53:00.261917Z","shell.execute_reply.started":"2024-01-03T15:52:52.975532Z","shell.execute_reply":"2024-01-03T15:53:00.261003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exercise: Variational Autoencoders (VAE) and Conditional Variational Autoencoders (CVAE)","metadata":{"id":"0FMl4JeoKDKL"}},{"cell_type":"markdown","source":"with MLP Architectures","metadata":{"id":"iQrFJUXXKF9p"}},{"cell_type":"markdown","source":"In this exercise, we'll explore Variational Autoencoders (VAE) and Conditional VAE (CVAE) using PyTorch.","metadata":{"id":"PnpL-pw9KG0C"}},{"cell_type":"markdown","source":"We will implement these models using Multi-Layer Perceptrons (MLP) ","metadata":{"id":"z5HWQihDKJh6"}},{"cell_type":"markdown","source":"Let's get started!","metadata":{"id":"rm823PxKKL06"}},{"cell_type":"markdown","source":"# 1) VAE --> MLP","metadata":{"id":"jjZJP48YKN5R"}},{"cell_type":"code","source":"# Define a simple VAE class with MLP architecture\n\nclass VAE_MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(VAE_MLP, self).__init__()\n        # TODO: Define the architecture of the encoder and decoder\n        self.encoder = nn.Sequential(\n            # TODO: Add layers for the encoder\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        self.decoder = nn.Sequential(\n            # TODO: Add layers for the decoder\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def reparameterize(self, mu, logvar):\n        # TODO: Implement the reparameterization trick\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        # TODO: Implement the forward pass\n            \n        x = x.view(-1, self.encoder[0].in_features)\n        # Encode\n        x = self.encoder(x)\n        # Sample\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        z = self.reparameterize(mu, logvar)\n        z = z.to(x.device)\n        # Decode\n        reconstructed = self.decoder(z)\n        return reconstructed, mu, logvar","metadata":{"id":"qtM6FO5cKSLr","execution":{"iopub.status.busy":"2024-01-03T15:53:00.263100Z","iopub.execute_input":"2024-01-03T15:53:00.263395Z","iopub.status.idle":"2024-01-03T15:53:00.273716Z","shell.execute_reply.started":"2024-01-03T15:53:00.263368Z","shell.execute_reply":"2024-01-03T15:53:00.272808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define VAE loss function\n\ndef vae_loss(recon, data, mu, logvar):\n    # TODO: Implement the reconstruction loss\n    # Training on MNIST, so we chose to use BCE loss\n    reconstruction_loss = F.binary_cross_entropy(recon, data, reduction = \"sum\")\n#     reconstruction_loss = F.mse_loss(recon, data)\n    # TODO: Implement the KL divergence loss\n    kl_divergence = - 0.5 * torch.sum(\n        1+ logvar - mu.pow(2) - logvar.exp()\n    )\n\n    # TODO: Return the total loss as the sum of reconstruction and KL divergence losses\n    return reconstruction_loss + kl_divergence","metadata":{"id":"wpJi7irWLCMJ","execution":{"iopub.status.busy":"2024-01-03T15:53:00.274882Z","iopub.execute_input":"2024-01-03T15:53:00.275704Z","iopub.status.idle":"2024-01-03T15:53:00.287704Z","shell.execute_reply.started":"2024-01-03T15:53:00.275671Z","shell.execute_reply":"2024-01-03T15:53:00.286894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop - VAE (MLP)\ndef train_vae_mlp(model, train_loader, num_epochs=10, learning_rate=1e-3):\n    model.train()\n    device = next(model.parameters()).device\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    best_loss = float('inf')  # Initialize with a high value\n    best_model = None\n\n    for epoch in range(num_epochs):\n        print()\n        print(50 * \"#\")\n        total_loss = 0\n        for batch_idx, (data, _) in enumerate(train_loader):\n\n            # TODO: Forward process\n            data = data.to(device)\n            optimizer.zero_grad()\n            \n            recon, mu, logvar = model(data)\n            # TODO: Flatten the data and recon tensors\n            data = data.view(-1)\n            recon = recon.view(-1)\n\n            # TODO: Calculate the loss using the vae_loss function\n            loss = vae_loss(recon, data, mu, logvar)\n\n            # TODO: Backpropagation and optimization step\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'VAE-MLP Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n\n        # Show some sample images after each epoch\n        if (epoch + 1) % 1 == 0:\n            print(\"Sample Images:\")\n            with torch.no_grad():\n                num_samples = 6  # Generate num_samples random samples\n#                 sample = torch.randn(num_samples, 20)\n                sample = torch.randn(num_samples, 2)\n                sample = sample.to(device)\n                sample = model.decoder(sample).view(num_samples, 1, 28, 28)\n                sample = sample.squeeze().cpu()\n                fig, axs = plt.subplots(1, num_samples, figsize=(15, 2))\n                for i in range(num_samples):\n                    axs[i].imshow(sample[i].cpu(), cmap='gray')\n                    axs[i].axis('off')\n                plt.show()\n\n        # TODO: Save the best model based on loss\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            best_model = model.state_dict()  # TODO: Save the model\n\n    # TODO: Save the best model to a file\n    torch.save(best_model, 'best_vae_mlp_model.pth')\n    print(\"Best model saved as 'best_vae_mlp_model.pth'\")\n","metadata":{"id":"VsOcJ986LU_R","execution":{"iopub.status.busy":"2024-01-03T15:53:00.288798Z","iopub.execute_input":"2024-01-03T15:53:00.289069Z","iopub.status.idle":"2024-01-03T15:53:00.302729Z","shell.execute_reply.started":"2024-01-03T15:53:00.289040Z","shell.execute_reply":"2024-01-03T15:53:00.301915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train VAE-MLP\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvae_mlp = VAE_MLP(x_dim, 512, 2)\nvae_mlp = vae_mlp.to(device)\ntrain_vae_mlp(vae_mlp, train_loader, learning_rate=0.001)","metadata":{"id":"WlKpLuOoLtdv","execution":{"iopub.status.busy":"2024-01-03T15:53:00.303791Z","iopub.execute_input":"2024-01-03T15:53:00.304089Z","iopub.status.idle":"2024-01-03T15:53:51.177473Z","shell.execute_reply.started":"2024-01-03T15:53:00.304063Z","shell.execute_reply":"2024-01-03T15:53:51.176073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) CVAE --> MLP","metadata":{"id":"vI7KhUKoPOTt"}},{"cell_type":"code","source":"# Define a simple CVAE class with MLP architecture\n\nclass CVAE_MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim, num_classes):\n        super(CVAE_MLP, self).__init__()\n        # TODO: Define the architecture of the encoder\n        self.encoder = nn.Sequential(\n            # TODO: Add layers for the encoder\n            nn.Linear(input_dim + num_classes, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        self.fc_class = nn.Linear(hidden_dim, num_classes)\n        self.decoder = nn.Sequential(\n            # TODO: Add layers for the decoder\n            nn.Linear(latent_dim + num_classes, hidden_dim),\n#             nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def reparameterize(self, mu, logvar):\n        # TODO: Implement the reparameterization trick\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x, y):\n        x = x.view(x.size(0), -1)\n        y = y.view(y.size(0), -1)\n\n        # TODO: Concatenate x and y before passing them to the encoder\n        x = torch.cat([x, y], dim=1)\n        \n        # TODO: Implement the forward pass\n        hidden = self.encoder(x)\n        mu = self.fc_mu(hidden)\n        logvar = self.fc_logvar(hidden)\n        z = self.reparameterize(mu, logvar)\n        class_logits = self.fc_class(hidden)\n        \n        z_cat = torch.cat([z, y], dim=1)     \n#         z_cat = z\n        reconstructed = self.decoder(z_cat)\n        \n        return reconstructed, mu, logvar, class_logits\n","metadata":{"id":"l0t8nce1PVmE","execution":{"iopub.status.busy":"2024-01-03T15:53:51.180325Z","iopub.execute_input":"2024-01-03T15:53:51.181286Z","iopub.status.idle":"2024-01-03T15:53:51.202962Z","shell.execute_reply.started":"2024-01-03T15:53:51.181226Z","shell.execute_reply":"2024-01-03T15:53:51.201834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define CVAE loss function\ndef cvae_loss(recon, data, mu, logvar, class_logits, labels):\n    # TODO: Flatten the data tensor\n    data = data.view(-1)\n    recon = recon.view(-1)\n\n    # TODO: Implement the reconstruction loss\n    reconstruction_loss = F.binary_cross_entropy(recon, data, reduction = \"sum\")\n\n    # TODO: Implement the KL divergence loss\n    kl_divergence = - 0.5 * torch.sum(\n        1+ logvar - mu.pow(2) - logvar.exp()\n    )\n\n    # TODO: Implement the cross-entropy loss for class prediction\n    ce_loss = F.cross_entropy(class_logits, labels, reduction=\"sum\")\n\n    # TODO: Return the total loss as the sum of reconstruction, KL divergence, and cross-entropy losses\n    return reconstruction_loss + kl_divergence + ce_loss\n","metadata":{"id":"c3xHfeQPhUB2","execution":{"iopub.status.busy":"2024-01-03T15:53:51.208134Z","iopub.execute_input":"2024-01-03T15:53:51.209442Z","iopub.status.idle":"2024-01-03T15:53:51.218129Z","shell.execute_reply.started":"2024-01-03T15:53:51.209397Z","shell.execute_reply":"2024-01-03T15:53:51.217538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop - CVAE (MLP)\ndef train_cvae_mlp(model, train_loader, num_epochs=10, learning_rate=1e-3):\n    model.train()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    device = next(model.parameters()).device\n    best_loss = float('inf')  # Initialize with a high value\n    best_model = None\n\n    for epoch in range(num_epochs):\n        print()\n        print(50 * \"#\")\n        total_loss = 0\n        for batch_idx, (data, labels) in enumerate(train_loader):\n            data, labels = data.to(device), labels.to(device)\n            optimizer.zero_grad()\n            # One-hot encode the labels\n            labels_one_hot = F.one_hot(labels, num_classes=-1)\n\n            # TODO: Forward pass through the model and calculate the loss using cvae_loss\n            recon, mu, logvar, class_logits = model(data, labels_one_hot)\n            loss = cvae_loss(recon, data, mu, logvar, class_logits, labels)\n\n            # TODO: Backpropagation and optimization step\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'CVAE-MLP Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n\n        # Show some sample images after each epoch\n        if (epoch + 1) % 1 == 0:\n            print(\"Sample Images:\")\n            with torch.no_grad():\n                num_classes = 10  # Number of classes (0 to 9)\n                num_samples_per_class = 1  # One sample per class\n                # Generate random samples\n                z = torch.randn(num_classes * num_samples_per_class, 2)\n                # Generate one-hot encoded class labels\n                y = torch.eye(num_classes).repeat(num_samples_per_class, 1)\n                # Concatenate the random samples and class labels before passing them to the decoder\n                sample = torch.cat([z, y], dim=1)\n                sample = sample.to(device)\n                sample = model.decoder(sample).view(num_classes * num_samples_per_class, 1, 28, 28)\n                sample = sample.squeeze().cpu()\n                fig, axs = plt.subplots(1, num_classes, figsize=(15, 2))\n                for i in range(num_classes):\n                    axs[i].imshow(sample[i], cmap='gray')\n                    axs[i].set_title(f\"Class {i}\", fontsize=16)\n                    axs[i].axis('off')\n                plt.show()\n\n        # TODO: Save the best model based on loss\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            best_model = model.state_dict()\n\n    # TODO: Save the best model to a file\n    torch.save(best_model, 'best_cvae_mlp_model.pth')\n    print(\"Best model saved as 'best_cvae_mlp_model.pth'\")\n","metadata":{"id":"7hYPGDSQhU99","execution":{"iopub.status.busy":"2024-01-03T15:53:51.219231Z","iopub.execute_input":"2024-01-03T15:53:51.219845Z","iopub.status.idle":"2024-01-03T15:53:51.233438Z","shell.execute_reply.started":"2024-01-03T15:53:51.219820Z","shell.execute_reply":"2024-01-03T15:53:51.232528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvae_mlp = CVAE_MLP(x_dim, 512, 2, 10)\ncvae_mlp = cvae_mlp.to(device)\ntrain_cvae_mlp(cvae_mlp, train_loader)","metadata":{"id":"9d7Q-iqjiVDc","execution":{"iopub.status.busy":"2024-01-03T15:53:51.234435Z","iopub.execute_input":"2024-01-03T15:53:51.234770Z","iopub.status.idle":"2024-01-03T15:54:45.010590Z","shell.execute_reply.started":"2024-01-03T15:53:51.234738Z","shell.execute_reply":"2024-01-03T15:54:45.009648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Visualizing Latent Space Distribution in Two Models\n\nIn this section, we will visualize the latent space distribution for two different models:\n\n## Latent Space Distribution:\n\n### VAE (MLP):\n\n1. Visualize the latent space distribution using Multilayer Perceptron (MLP) in VAE model.\n2. Analyze the characteristics of the distribution.\n\n### CVAE (MLP):\n\n1. Visualize the latent space distribution using Multilayer Perceptron (MLP) in CVAE model.\n2. Analyze the characteristics of the distribution.\n\n\nSummarize your findings and draw conclusions based on the observed differences in the latent space distribution between VAE (MLP) and CVAE (MLP).\n","metadata":{"id":"vYB4Id3VPxDF"}},{"cell_type":"code","source":"vae_mlp.eval()\nlatent_points = []\nlabels = []\n\nwith torch.no_grad():\n    for i, (data, label) in enumerate(train_loader):\n        data = data.to(device)\n        recon, mu, _ = vae_mlp(data)\n\n        latent_points.append(mu.cpu().numpy())\n        labels.append(label.numpy())\n        \n        if i == 0:\n            fig, axs = plt.subplots(1, 2, figsize=(5, 3))\n            axs[0].imshow(data[0].squeeze().cpu(), cmap='gray')\n            axs[0].axis('off')\n            axs[1].imshow(recon.cpu()[0].reshape(shape=data[0].shape).squeeze().cpu(), cmap='gray')\n            axs[1].axis('off')\n            plt.show()\n\nlatent_points = np.concatenate(latent_points, axis=0)\nlabels = np.concatenate(labels, axis=0)","metadata":{"id":"bGYNnqb4DbIk","execution":{"iopub.status.busy":"2024-01-03T15:54:45.011951Z","iopub.execute_input":"2024-01-03T15:54:45.012216Z","iopub.status.idle":"2024-01-03T15:54:49.041358Z","shell.execute_reply.started":"2024-01-03T15:54:45.012186Z","shell.execute_reply":"2024-01-03T15:54:49.040314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create subplots\nrows, cols = 2, 5\nfig, axes = plt.subplots(rows, cols, figsize=(15, 7))\n\n# Iterate over each class label (0 to 9) and plot in the corresponding subplot\nfor class_label in range(10):\n    # Select latent points and labels for the current class\n    class_indices = np.where(labels == class_label)[0]\n    class_latent_points = latent_points[class_indices]\n\n    # Determine the subplot position\n    row = class_label // cols\n    col = class_label % cols\n\n    # Plotting in the subplot\n    scatter = axes[row, col].scatter(class_latent_points[:, 0], class_latent_points[:, 1], alpha=0.7)\n    axes[row, col].set_title(f'Class {class_label}')\n    axes[row, col].set_xlabel('Latent Dimension 1')\n    axes[row, col].set_ylabel('Latent Dimension 2')\n\n# Adjust layout to prevent clipping of titles\nplt.tight_layout()\nplt.show()\n\n# Plotting\nplt.figure(figsize=(20, 10))\nscatter = plt.scatter(latent_points[:, 0], latent_points[:, 1], c=labels, cmap='viridis', alpha=0.7)\nplt.colorbar(scatter, label='Class')\nplt.title('Shared Latent Space Distribution')\nplt.xlabel('Latent Dimension 1')\nplt.ylabel('Latent Dimension 2')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T15:54:49.042868Z","iopub.execute_input":"2024-01-03T15:54:49.043151Z","iopub.status.idle":"2024-01-03T15:54:52.779457Z","shell.execute_reply.started":"2024-01-03T15:54:49.043120Z","shell.execute_reply":"2024-01-03T15:54:52.778485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvae_mlp.eval()\nlatent_points = []\nlabels = []\n\nwith torch.no_grad():\n    for i, (data, label) in enumerate(train_loader):\n        data, label = data.to(device), label.to(device)\n        labels_one_hot = F.one_hot(label, num_classes=-1)\n        recon, mu, logvar, class_logits = cvae_mlp(data, labels_one_hot)\n\n        latent_points.append(mu.cpu().numpy())\n        labels.append(label.cpu().numpy())\n        \n        if i == 0:\n            fig, axs = plt.subplots(1, 2, figsize=(5, 3))\n            axs[0].imshow(data[0].squeeze().cpu(), cmap='gray')\n            axs[0].axis('off')\n            axs[1].imshow(recon.cpu()[0].reshape(shape=data[0].shape).squeeze().cpu(), cmap='gray')\n            axs[1].axis('off')\n            plt.show()\n\nlatent_points = np.concatenate(latent_points, axis=0)\nlabels = np.concatenate(labels, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T15:54:52.780561Z","iopub.execute_input":"2024-01-03T15:54:52.780871Z","iopub.status.idle":"2024-01-03T15:54:56.801881Z","shell.execute_reply.started":"2024-01-03T15:54:52.780846Z","shell.execute_reply":"2024-01-03T15:54:56.800773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create subplots\nrows, cols = 2, 5\nfig, axes = plt.subplots(rows, cols, figsize=(15, 7))\n\n# Iterate over each class label (0 to 9) and plot in the corresponding subplot\nfor class_label in range(10):\n    # Select latent points and labels for the current class\n    class_indices = np.where(labels == class_label)[0]\n    class_latent_points = latent_points[class_indices]\n\n    # Determine the subplot position\n    row = class_label // cols\n    col = class_label % cols\n\n    # Plotting in the subplot\n    scatter = axes[row, col].scatter(class_latent_points[:, 0], class_latent_points[:, 1], alpha=0.7)\n    axes[row, col].set_title(f'Class {class_label}')\n    axes[row, col].set_xlabel('Latent Dimension 1')\n    axes[row, col].set_ylabel('Latent Dimension 2')\n\n# Adjust layout to prevent clipping of titles\nplt.tight_layout()\nplt.show()\n\n# Plotting\nplt.figure(figsize=(20, 10))\nscatter = plt.scatter(latent_points[:, 0], latent_points[:, 1], c=labels, cmap='viridis', alpha=0.7)\nplt.colorbar(scatter, label='Class')\nplt.title('Shared Latent Space Distribution')\nplt.xlabel('Latent Dimension 1')\nplt.ylabel('Latent Dimension 2')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T15:54:56.803787Z","iopub.execute_input":"2024-01-03T15:54:56.804124Z","iopub.status.idle":"2024-01-03T15:55:00.791951Z","shell.execute_reply.started":"2024-01-03T15:54:56.804091Z","shell.execute_reply":"2024-01-03T15:55:00.790905Z"},"trusted":true},"execution_count":null,"outputs":[]}]}