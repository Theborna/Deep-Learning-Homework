{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torchvision\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"using {device} device\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-10T18:09:02.356426Z","iopub.execute_input":"2024-01-10T18:09:02.356864Z","iopub.status.idle":"2024-01-10T18:09:02.365480Z","shell.execute_reply.started":"2024-01-10T18:09:02.356832Z","shell.execute_reply":"2024-01-10T18:09:02.364383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n\nbatch_size = 256\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:09:02.367993Z","iopub.execute_input":"2024-01-10T18:09:02.368934Z","iopub.status.idle":"2024-01-10T18:09:02.485541Z","shell.execute_reply.started":"2024-01-10T18:09:02.368899Z","shell.execute_reply":"2024-01-10T18:09:02.484637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder and Decoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, output_dim=512):\n        super(Encoder, self).__init__()\n        # Conv layers\n        self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=4, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.conv4 = nn.Conv2d(256, output_dim, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(output_dim)\n        # Bottleneck conv\n        self.residual_conv1 = nn.Conv2d(128, 256, kernel_size=1)\n        self.residual_conv2 = nn.Conv2d(256, output_dim, kernel_size=1)\n        # Activation\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        # First block\n        x = self.act(self.bn1(self.conv1(x)))\n        x = self.act(self.bn2(self.conv2(x)))\n        # Second block\n        res = self.residual_conv1(x)\n        x = self.act(self.bn3(self.conv3(x)))\n        x = res + x\n        res = self.residual_conv2(x)\n        x = self.act(self.bn4(self.conv4(x)))\n        y = res + x\n        return y\n\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_dim, output_dim=1):\n        super(Decoder, self).__init__()\n        # Transposed Conv layers\n        self.deconv4 = nn.ConvTranspose2d(input_dim, 256, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.deconv1 = nn.ConvTranspose2d(64, output_dim, kernel_size=4, stride=2, padding=1)\n        # Activation\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        # Third block\n        x = self.act(self.bn4(self.deconv4(x)))\n        # Second block\n        x = self.act(self.bn3(self.deconv3(x)))\n        # First block\n        x = self.act(self.bn2(self.deconv2(x)))\n        y = self.deconv1(x)\n        return y\n\n# Test \nenc = Encoder(1)\ndec = Decoder(512)\nfor x, _ in train_loader:\n    z = enc(x)\n    y = dec(z)\n    print(y.shape)\n    print(f\"Encoded data shape: {z.shape}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:09:02.487129Z","iopub.execute_input":"2024-01-10T18:09:02.487720Z","iopub.status.idle":"2024-01-10T18:09:03.645642Z","shell.execute_reply.started":"2024-01-10T18:09:02.487690Z","shell.execute_reply":"2024-01-10T18:09:03.644371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim):\n        super(VectorQuantizer, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n        nn.init.uniform_(self.embedding.weight, -1.0, 1.0)\n        \n    def forward(self, x):\n        # Flatten input tensor\n        x = x.permute(0, 2, 3, 1).contiguous()\n        x_flat = x.view(-1, self.embedding_dim)\n        \n        # Find nearest embeddings\n        distances = torch.sum(x_flat ** 2, dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n            torch.matmul(x_flat, self.embedding.weight.t())\n        indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(indices.shape[0], self.num_embeddings).to(x.device)\n        encodings.scatter_(1, indices, 1)\n\n        # Quantize input\n        quantized = torch.matmul(encodings, self.embedding.weight).view(x.shape)\n\n        # Losses\n        codebook_loss = F.mse_loss(x.detach(), quantized)\n        commitment_loss = F.mse_loss(x, quantized.detach())\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n        return quantized, commitment_loss, codebook_loss, perplexity","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:09:03.647238Z","iopub.execute_input":"2024-01-10T18:09:03.647589Z","iopub.status.idle":"2024-01-10T18:09:03.659728Z","shell.execute_reply.started":"2024-01-10T18:09:03.647560Z","shell.execute_reply":"2024-01-10T18:09:03.658700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQ_VAE(nn.Module):\n    def __init__(self, Encoder, Codebook, Decoder):\n        super(VQ_VAE, self).__init__()\n        self.encoder = Encoder\n        self.codebook = Codebook\n        self.decoder = Decoder\n                \n    def forward(self, x):\n        z = self.encoder(x)\n        z_quantized, commitment_loss, codebook_loss, perplexity = self.codebook(z)\n        z_grad_flow = z + (z_quantized - z).detach()\n        x_hat = self.decoder(z_grad_flow)\n        \n        return x_hat, commitment_loss, codebook_loss, perplexity","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:09:03.662413Z","iopub.execute_input":"2024-01-10T18:09:03.662713Z","iopub.status.idle":"2024-01-10T18:09:03.671493Z","shell.execute_reply.started":"2024-01-10T18:09:03.662688Z","shell.execute_reply":"2024-01-10T18:09:03.670710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_embeddings = 3\nembedding_dim = 2\n\nencoder = Encoder(input_dim=1, output_dim=embedding_dim)\ncodebook = VectorQuantizer(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\ndecoder = Decoder(input_dim=embedding_dim, output_dim=1)\n\nmodel = VQ_VAE(Encoder=encoder, Codebook=codebook, Decoder=decoder).to(device)\nrecon = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=5e-4)\n\nbeta = 0.25\n\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:09:03.672684Z","iopub.execute_input":"2024-01-10T18:09:03.673112Z","iopub.status.idle":"2024-01-10T18:09:03.706827Z","shell.execute_reply.started":"2024-01-10T18:09:03.673079Z","shell.execute_reply":"2024-01-10T18:09:03.705928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 30\nprint_idx = 2\n\nmodel.train()\ntrain_losses = []\ntrain_perplexities = []\ncodebook_vectors_list = []\n\nfor epoch in range(num_epochs):\n    overall_loss = 0\n    overall_perplexity = 0\n\n    for (x, _) in tqdm(train_loader, desc=f'Training epoch {epoch+1}/{num_epochs}'):\n        x = x.to(device)\n\n        optimizer.zero_grad()\n\n        x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n        recon_loss = recon(x_hat, x)\n        \n        loss = recon_loss + codebook_loss + beta * commitment_loss\n        \n        overall_loss += loss.item()\n        overall_perplexity += perplexity.item()\n\n        loss.backward()\n        optimizer.step()\n\n    # Calculate mean loss and mean perplexity for the epoch\n    mean_loss = overall_loss / len(train_loader)\n    mean_perplexity = overall_perplexity / len(train_loader)\n    codebook_vectors = model.codebook.embedding.weight.data.cpu().numpy()\n    \n    # Print metrics once every two epochs\n    if (epoch + 1) % print_idx == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Mean Loss: {mean_loss:.4f}, Mean Perplexity: {mean_perplexity:.4f}')\n\n    # Save metrics for plotting\n    train_losses.append(mean_loss)\n    train_perplexities.append(mean_perplexity)\n    codebook_vectors_list.append(codebook_vectors)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:09:03.708027Z","iopub.execute_input":"2024-01-10T18:09:03.708289Z","iopub.status.idle":"2024-01-10T18:11:24.977530Z","shell.execute_reply.started":"2024-01-10T18:09:03.708265Z","shell.execute_reply":"2024-01-10T18:11:24.976300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_perplexities, label='Training Perplexity')\nplt.title('Training Perplexity Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Perplexity')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:11:24.979207Z","iopub.execute_input":"2024-01-10T18:11:24.979543Z","iopub.status.idle":"2024-01-10T18:11:25.571933Z","shell.execute_reply.started":"2024-01-10T18:11:24.979515Z","shell.execute_reply":"2024-01-10T18:11:25.571045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nmodel.eval()\n\nnum_examples_to_plot = 5\n\ntest_examples = next(iter(train_loader))[:num_examples_to_plot]\ninputs, _ = test_examples\ninputs = inputs.to(device)\n\n# Generate outputs\nwith torch.no_grad():\n    encoded = encoder(inputs)\n    codebook_output, _, _, _ = codebook(encoded) \n    outputs = decoder(codebook_output)\n\ninputs = inputs.cpu().numpy()\noutputs = outputs.cpu().numpy()\n# codebook_output = codebook_output\npad = ((codebook_output[:, 0:1, :, :]) + (codebook_output[:, 1:2, :, :])) / 2\n# codebook_output = torch.cat([pad, codebook_output], dim=1)\ncodebook_output = pad\ncodebook_output = codebook_output.cpu().numpy()\nembedding_vectors = model.codebook.embedding.weight.data.cpu().numpy()\n\n# Plot examples\nplt.figure(figsize=(3 * num_examples_to_plot, 6))\n\nfor example_idx in range(num_examples_to_plot):\n    plt.subplot(3, num_examples_to_plot, 1 + example_idx)\n    plt.imshow(inputs[example_idx].transpose(1, 2, 0), cmap='Greys')\n    plt.title(f'Example {example_idx + 1}\\nInput')\n\n    plt.subplot(3, num_examples_to_plot, 1 + example_idx + num_examples_to_plot)\n    plt.imshow(outputs[example_idx].transpose(1, 2, 0), cmap='Greys')\n    plt.title(f'Output')\n    \n    plt.subplot(3, num_examples_to_plot, 1 + example_idx + 2 * num_examples_to_plot)\n    plt.imshow(codebook_output[example_idx].transpose(1, 2, 0),cmap='viridis')\n    plt.title(f'Codebook Output')\n# Plot codebook vectors separately\nplt.tight_layout()\nplt.show()\n\n# Plot codebook vectors\ncolors = plt.cm.viridis(np.linspace(0, 1, len(embedding_vectors)))\nplt.figure(figsize=(8, 8))\n\norigin = np.array([np.zeros_like(embedding_vectors[:, 0]), np.zeros_like(embedding_vectors[:, 1])]) # origin point\nplt.quiver(*origin, embedding_vectors[:, 0], embedding_vectors[:, 1], color=colors, scale_units='xy', angles='xy',\n           scale=1)\nplt.xlim(-1.5, 1.5)\nplt.ylim(-1.5, 1.5)\nplt.title('Codebook Vectors')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:40.001592Z","iopub.execute_input":"2024-01-10T18:13:40.002071Z","iopub.status.idle":"2024-01-10T18:13:43.002073Z","shell.execute_reply.started":"2024-01-10T18:13:40.002028Z","shell.execute_reply":"2024-01-10T18:13:43.000630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.animation import FuncAnimation\n\n# Create a GIF\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.title('Codebook Vectors Over Training')\ncolors = plt.cm.viridis(np.linspace(0, 1, len(embedding_vectors)))\n\ndef update(frame):\n    ax.clear()\n    ax.quiver(np.zeros_like(codebook_vectors_list[frame][:, 0]),\n              np.zeros_like(codebook_vectors_list[frame][:, 1]),\n              codebook_vectors_list[frame][:, 0], codebook_vectors_list[frame][:, 1],\n              angles='xy', scale_units='xy', scale=1, color=colors)\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_title(f'Epoch {frame + 1}')\n\nani = FuncAnimation(fig, update, frames=len(codebook_vectors_list), repeat=False)\nani.save('codebook_vectors_animation.gif', writer='imagemagick', fps=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:55.933348Z","iopub.execute_input":"2024-01-10T18:13:55.934299Z","iopub.status.idle":"2024-01-10T18:14:18.095237Z","shell.execute_reply.started":"2024-01-10T18:13:55.934265Z","shell.execute_reply":"2024-01-10T18:14:18.094045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing different sizes","metadata":{}},{"cell_type":"markdown","source":"## Coloured dataset","metadata":{}},{"cell_type":"code","source":"class ColouredMNIST(Dataset):\n    def __init__(self, root, train=True, transform=None, download=True, random_seed=42):\n        self.mnist_dataset = datasets.MNIST(root=root, train=train, transform=transform, download=download)\n        self.random_seed = random_seed\n        self.random_state = torch.manual_seed(self.random_seed)\n\n    def __len__(self):\n        return len(self.mnist_dataset)\n\n    def __getitem__(self, idx):\n        # Set the random seed for reproducibility\n        torch.manual_seed(self.random_seed * idx) # to make idx affect the colour\n        original_data, target = self.mnist_dataset[idx]\n        random_multipliers = torch.rand(3)\n        three_channel_data = torch.cat([\n            original_data * random_multipliers[0],\n            original_data * random_multipliers[1],\n            original_data * random_multipliers[2]],\n            dim=0,\n        )\n        return three_channel_data, target\n\n# Example\ntransform = transforms.ToTensor()\ncoloured_dataset = ColouredMNIST(root='./data', train=True, transform=transform, download=True)\nbatch_size = 256\ncoloured_loader = DataLoader(coloured_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=False)\n\n# Access an example from the custom dataset\nexample_data, _ = coloured_dataset[10]\nprint(\"Example Data Shape:\", example_data.shape)\nplt.imshow(example_data.cpu().numpy().transpose(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:11:51.128517Z","iopub.execute_input":"2024-01-10T18:11:51.129528Z","iopub.status.idle":"2024-01-10T18:11:51.485208Z","shell.execute_reply.started":"2024-01-10T18:11:51.129479Z","shell.execute_reply":"2024-01-10T18:11:51.484273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def VQ_VAE_trained_model(data_loader, input_dim, num_embeddings, embedding_dim,\n                         device='cpu', num_epochs=30, print_idx=10, print_metric=False):\n    encoder = Encoder(input_dim=input_dim, output_dim=embedding_dim)\n    codebook = VectorQuantizer(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n    decoder = Decoder(input_dim=embedding_dim, output_dim=input_dim)\n\n    model = VQ_VAE(Encoder=encoder, Codebook=codebook, Decoder=decoder).to(device)\n    recon = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n    beta = 0.25\n\n    model.train()\n    train_losses = []\n    train_perplexities = []\n\n    for epoch in tqdm(range(num_epochs), \n                      desc=f'Training model with {num_embeddings}, {embedding_dim} dimentional embeddings'):\n        overall_loss = 0\n        overall_perplexity = 0\n\n        for (x, _) in (data_loader):\n            x = x.to(device)\n            optimizer.zero_grad()\n\n            x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n            recon_loss = recon(x_hat, x)\n\n            loss = recon_loss + codebook_loss + beta * commitment_loss\n\n            overall_loss += loss.item()\n            overall_perplexity += perplexity.item()\n\n            loss.backward()\n            optimizer.step()\n\n        # Calculate mean loss and mean perplexity for the epoch\n        mean_loss = overall_loss / len(train_loader)\n        mean_perplexity = overall_perplexity / len(train_loader)\n\n        # Print metrics once every two epochs\n        if print_metric:\n            if (epoch + 1) % print_idx == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Mean Loss: {mean_loss:.4f}, Mean Perplexity: {mean_perplexity:.4f}')\n\n        # Save metrics for plotting\n        train_losses.append(mean_loss)\n        train_perplexities.append(mean_perplexity)\n    \n    metrics = {\n        'loss': train_losses,\n        'perplexity': train_perplexities,\n    }\n    \n    return model, metrics","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:11:51.486834Z","iopub.execute_input":"2024-01-10T18:11:51.487131Z","iopub.status.idle":"2024-01-10T18:11:51.501324Z","shell.execute_reply.started":"2024-01-10T18:11:51.487105Z","shell.execute_reply":"2024-01-10T18:11:51.500390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_model_output(model, data_loader, ax=None):\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    model.eval()\n\n    num_examples_to_plot = 5\n\n    test_examples = next(iter(data_loader))[:num_examples_to_plot]\n    inputs, _ = test_examples\n    inputs = inputs.to(device)\n\n    # Generate outputs\n    with torch.no_grad():\n        outputs, _, _, _ = model(inputs)\n\n    inputs = inputs.cpu().numpy()\n    outputs = outputs.cpu().numpy()\n\n    embedding_vectors = model.codebook.embedding.weight.data.cpu().numpy()\n\n    if ax is None:\n        fig, ax = plt.subplots(2, num_examples_to_plot, figsize=(15, 6))\n\n    for example_idx in range(num_examples_to_plot):\n        ax[0, example_idx].imshow(inputs[example_idx].transpose(1, 2, 0))\n        ax[0, example_idx].set_title(f'Example {example_idx + 1}\\nInput')\n\n        ax[1, example_idx].imshow(outputs[example_idx].transpose(1, 2, 0))\n        ax[1, example_idx].set_title(f'Output')\n\n    return ax\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:11:51.502635Z","iopub.execute_input":"2024-01-10T18:11:51.502964Z","iopub.status.idle":"2024-01-10T18:11:51.513567Z","shell.execute_reply.started":"2024-01-10T18:11:51.502934Z","shell.execute_reply":"2024-01-10T18:11:51.512503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Effect of number of embeddings","metadata":{}},{"cell_type":"code","source":"NumEmbeddings = [3, 5, 10, 20, 30, 40, 50]\nmodelsNum = [\n    VQ_VAE_trained_model(coloured_loader, 3, num, 3, device) for num in NumEmbeddings\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:14:49.654883Z","iopub.execute_input":"2024-01-10T18:14:49.655285Z","iopub.status.idle":"2024-01-10T18:37:06.556469Z","shell.execute_reply.started":"2024-01-10T18:14:49.655246Z","shell.execute_reply":"2024-01-10T18:37:06.555246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, (model, metrics) in enumerate(modelsNum):\n    print(f'Model with {NumEmbeddings[idx]} embedding vectors')\n    final_loss = metrics['loss'][-1]\n    final_perplexity = metrics['perplexity'][-1]\n    print(f'final loss: {final_loss}')\n    print(f'final perplexity: {final_perplexity}')\n    sample_model_output(model, coloured_loader)\n    plt.tight_layout()\n    plt.show()\n    \nfrom matplotlib.animation import FuncAnimation\n\n# Create a GIF\nfig, ax = plt.subplots(2, 5, figsize=(15, 6))\nplt.title('Model Output Over Models')\ncolors = plt.cm.viridis(np.linspace(0, 1, len(modelsNum)))\n\ndef update(frame):\n    model, metrics = modelsNum[frame]\n    final_loss = metrics['loss'][-1]\n    final_perplexity = metrics['perplexity'][-1]\n    \n    sample_model_output(model, coloured_loader, ax)\n    fig.suptitle(f'Number of embeddings: {NumEmbeddings[frame]}')\n\nani = FuncAnimation(fig, update, frames=len(modelsNum), repeat=False)\nani.save('model_output_num_embedding_animation.gif', writer='imagemagick', fps=2)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:37:06.559010Z","iopub.execute_input":"2024-01-10T18:37:06.559354Z","iopub.status.idle":"2024-01-10T18:37:35.985691Z","shell.execute_reply.started":"2024-01-10T18:37:06.559305Z","shell.execute_reply":"2024-01-10T18:37:35.984487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DimEmbeddings = [2, 5, 10, 20, 30, 40, 50]\nmodelsDim = [\n    VQ_VAE_trained_model(coloured_loader, 3, 3, dim, device) for dim in DimEmbeddings\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:37:35.987354Z","iopub.execute_input":"2024-01-10T18:37:35.987650Z","iopub.status.idle":"2024-01-10T18:59:54.939748Z","shell.execute_reply.started":"2024-01-10T18:37:35.987623Z","shell.execute_reply":"2024-01-10T18:59:54.938537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, (model, metrics) in enumerate(modelsDim):\n    print(f'Model with {DimEmbeddings[idx]} embedding dimension')\n    final_loss = metrics['loss'][-1]\n    final_perplexity = metrics['perplexity'][-1]\n    print(f'final loss: {final_loss}')\n    print(f'final perplexity: {final_perplexity}')\n    sample_model_output(model, coloured_loader)\n    plt.tight_layout()\n    plt.show()\n    \nfrom matplotlib.animation import FuncAnimation\n\n# Create a GIF\nfig, ax = plt.subplots(2, 5, figsize=(15, 6))\nplt.title('Model Output Over Models')\ncolors = plt.cm.viridis(np.linspace(0, 1, len(modelsDim)))\n\ndef update(frame):\n    model, metrics = modelsDim[frame]\n    final_loss = metrics['loss'][-1]\n    final_perplexity = metrics['perplexity'][-1]\n    \n    sample_model_output(model, coloured_loader, ax)\n    fig.suptitle(f'embedding dimension: {DimEmbeddings[frame]}')\n\nani = FuncAnimation(fig, update, frames=len(modelsNum), repeat=False)\nani.save('model_output_dim_embedding_animation.gif', writer='imagemagick', fps=2)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:59:54.941528Z","iopub.execute_input":"2024-01-10T18:59:54.941880Z","iopub.status.idle":"2024-01-10T19:00:24.486229Z","shell.execute_reply.started":"2024-01-10T18:59:54.941850Z","shell.execute_reply":"2024-01-10T19:00:24.485071Z"},"trusted":true},"execution_count":null,"outputs":[]}]}