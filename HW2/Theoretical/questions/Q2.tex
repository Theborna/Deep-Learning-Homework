\section{سوال 2}

می دانیم حتی یک شبکه عصبی یک لایه نیز می تواند طبقه بندی ارقام را با دقت خوبی انجام
دهد. راه های متعددی برای ارتقای دقت مدل وجود دارد.این
\href{http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520b250.pdf}{مقاله}
روشی ساده برای ارتقای عملکرد مدل،
بدون تغییر در ساختار آن پیشنهاد می دهد. این پیشنهاد آموزش چند مدل مشابه است. مقاله را بخوانید و به
سوالات زیر پاسخ دهید:

\begin{enumerate}
	\item {
	      کمیته چیست و چطور به بهبود عملکرد مدل کمک میکند؟

	      \begin{qsolve}[]
		      به طور خلاصه، کمیته به یک مجموعه از چندین طبقه‌بند مانند شبکه‌های عصبی اشاره دارد. آموزش این طبقه‌بندها روی نسخه‌های مختلفی از داده منجر به ایجاد مدل‌هایی می‌شود که خطاهای مختلفی دارند. ترکیب خروجی‌های آن‌ها از این تنوع بهره می‌برد؛ هر مدل اطلاعات تکمیلی ارائه می‌دهد که عملکرد کلی را بهبود می‌بخشد. کلید اینجا این است که خطاها به گونه‌ای تا حد امکان از یکدیگر مستقل باشند. این امر به کمیته اجازه می‌دهد خطاهای انفرادی مدل‌ها را اصلاح کند.

		      \[
			      y_{com}^k(x)=\sum_{n=1}^{N}w_{nk}y_{nk}(x)
		      \]

		      که در اینجا هر کدام از $y_{nk}(x)$ ها یک طبقه بند است، در کمیته ای با خطا های مستقل، خطای کل کمینه میشود.
	      \end{qsolve}
	      }
	\item {
	      پیش پردازش انجام شده در مقاله را شرح دهید. چگونه این پیش پردازش از وابستگی زیاد خطای مدل
	      ها جلوگیری می کند؟

	      \begin{qsolve}[]
		      تکنیک‌های کلیدی پیش‌پردازش داده که در این مقاله برای دی‌کرله کردن خطاها در میان مدل‌ها استفاده شده‌اند عبارتند از:

              \begin{itemize}
                \item \textbf{نرمالیزه کردن ابعاد اعداد: } آموزش مدل‌های مختلف بر روی داده‌هایی با \lr{bounding-box} به عرض متفاوت از 8 تا 20 پیکسل، تنوعی در نسبت ابعاد وجود دارد. این باعث می‌شود هر مدل بر اساس ویژگی‌های مختلف مرتبط با نسبت ابعاد تکمیلی اعتماد کند، که منجر به خطاهای غیرهمبسته می‌شود.
                \item \textbf{رفع انحنای ارقام: } رفع انحنا باعث می‌شود که اصلی‌ترین مولفه عمودی شود. این باعث کاهش انحراف در انحنا در میان نمونه‌ها می‌شود. مدل‌هایی که بر روی داده‌های رفع انحنا آموزش داده شده‌اند، خطاهای متفاوتی نسبت به مدل‌های آموزش داده شده بر روی داده‌های انحراف دارند.
                \item \textbf{انحراف الاستیک: } اعمال انحراف‌های مختلف خطی و انعطافی در طول آموزش مدل‌ها را نسبت به تغییرات مقاوم می‌کند. استفاده از پارامترهای مختلف انحراف برای هر مدل تنوع در داده‌های آموزش را افزایش می‌دهد و منجر به خطاهای غیرهمبسته می‌شود.
                \item \textbf{مجموعه‌های آموزش مختلف: } برخی از مدل‌ها بر روی زیرمجموعه‌های تصادفی مختلفی از داده‌های آموزش آموزش می‌بینند. این کار باعث دی‌کرله کردن خطاها بین مدل‌ها می‌شود.
              \end{itemize}

		      به طور خلاصه، با تغییراتی که هر مدل از طریق پیش‌پردازش و انحراف‌ها مشاهده می‌کند، آنها بر تمرکز بر روی یادگیری ویژگی‌ها و تثبیت‌های مختلف تمرکز می‌کنند. این تنوع در چیزهایی که یاد می‌گیرند منجر به خطاهای غیرهمبسته در مثال‌های تست می‌شود. ترکیب این مدل‌های متنوع باعث پوشش خطاهای یکدیگر می‌شود و عملکرد کلی را بهبود می‌بخشد.
	      \end{qsolve}
	      }
\end{enumerate}